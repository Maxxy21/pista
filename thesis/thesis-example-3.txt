Faculty of Engineering
Faculty of Economics and Management
Bachelor in Informatics and Management of Digital Business

Bachelor Thesis

Retrieval-Augmented Generation
Chatbot: A Proof-of-Concept
Implementation for Customer Service at
Durst Group AG

Author: Jonas Gatterer
Thesis supervisor: Prof. Dr. Oswald Lanz
March 2024

Abstract
The term LLM (large language model) has gained much popularity
over the last few months. Software applications like ChatGPT or Midjourney have completely changed the way we interact with AI. The sector
has experienced a huge amount of innovation and disruption, and new
use cases are emerging everywhere. One such use case is a chatbot for
the customer service department, to reduce operational costs and free-up
time for employees. This bachelor thesis will address my proof of concept
implementation for the company Durst Group AG. I will first provide a
theoretical overview for the technologies used in LLMs. This includes
the transformer model, LoRA and quantization, and RAG (retrievalaugmented generation). LLMs alone cannot always answer nuanced technical questions. I will present my proof of concept implementation that
uses the RAG architecture and achieves notably accurate answers for complex questions.

Contents
1 Introduction

3

2 Theoretical Background
2.1 Fundamentals of Transformer Models . . . . . . . . . . . . . . . .
2.1.1 The Attention Mechanism . . . . . . . . . . . . . . . . . .
2.1.2 Non-sequential Processes and Positional Encoding . . . .
2.2 Quantization and LoRA . . . . . . . . . . . . . . . . . . . . . . .
2.3 Overview of Retrieval-Augmented Generation (RAG) . . . . . . .

3
4
5
6
7
8

3 Technical Framework and Deployment Process
10
3.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Data Collection and Processing . . . . . . . . . . . . . . . . . . . 12
3.3 Cloud Implementation and Deployment . . . . . . . . . . . . . . 13
3.4 Software Development and Testing Methodology . . . . . . . . . 14
4 Evaluation of Results

14

5 Discussion

15

6 Conclusion and Future Work

16

A Code snippets

17

2

1

Introduction

Today, large language models can perform various tasks, such as translating
texts, generating texts or participating in conversations. During the pre-training
phase, a LLM is exposed to a large quantity of data that allows the LLM to
develop an understanding of language. The next phase is fine-tuning, where
a LLM is specialized for one or more tasks. With fine-tuning, a LLM can
learn to handle complex tasks. To carry out the fine-tuning process well, one
needs a lot of data and a large quantity of computing power. However, these
conditions are often not feasible for building a small chatbot. A solution to
this problem are retrieval-augmented generation (RAG) chatbots. RAG uses
an external knowledge base together with LLMs to answer questions or fulfil
user tasks and can thus make fine-tuning obsolete. In this bachelor thesis I will
present my proof-of-concept implementation for a RAG chatbot that I built for
the company Durst Group AG.
Durst Group AG is a manufacturer for digital printing and production technologies. The company focuses on the inkjet technology and offers solutions
for textile, ceramic, label and large format printing. The solutions include the
printer itself, inks, software and services. Durst Group AG is an international
company with subsidiaries in the USA, China, Australia and other places of
the world. My thesis internship at Durst Group AG lasted approximately 4
months, from the beginning of October 2023 to mid-February 2024. During this
internship, I built a generative AI chatbot for the company’s customer service
department. It is an internal tool for service technicians that is initially intended to relieve employees in the customer service department of simple and
repetitive questions so that they can concentrate on more complicated tasks.
My bachelor thesis first starts by covering the theoretical background of
LLMs. More specifically, I will explain how the fundamental aspects of a transformer model work. As I have also experimented with fine-tuning a LLM, I will
elaborate on helpful technologies such as LoRA and quantization. The theory
section ends with an introduction to the RAG approach. In Section 3 I will discuss the actual implementation of my POC chatbot. This includes the system
architecture, the data collection and processing process, the implementation and
deployment, and the software development and testing methodology. Lastly, I
will evaluate the conversational quality of the chatbot and discuss the results.

2

Theoretical Background

Before diving into the architecture of my project, I would like to give a thorough
explanation of the underlying technology. First, I will explain the fundamental
aspects of transformer models. They play an important role in the current trend
of artificial intelligence and deep learning. Transformers revolutionized the field
of natural language processing (NLP) by introducing a new architecture that
uses non-sequential processes, parallelizable attention, and positional encodings.
Further, I will describe how QLoRA, or also known as Quantized Low-Rank
Adaptation, works and why it makes the finetuning of LLMs (Large Language
Models) much more efficient. Lastly, I will give a brief overview of what a RAG
system is.

3

2.1

Fundamentals of Transformer Models

As I already mentioned before, the innovative architecture of transformers completely revolutionized NLP. The transformer model is the backbone of applications like ChatGPT, and it is used in every state-of-the-art chatbot application.
Today, transformer models are not only used in NLP, but also in computer
vision and even for audio models.

Figure 1: The transformer model architecture proposed by Vaswani et al. [4].
As one can see in Figure 1, the left side of the transformer model is the
encoder, where the inputs get encoded. The right side is the decoder block,
where the output sequence is generated. During the training phase, the encoder
block receives the input sequence, and the decoder block receives the target or
solution. During inference, the encoder block receives the input sequence again.
The decoder instead starts with a start token. The output of the decoder gets
then concatenated with the previous input, and it is passed to the decoder as
the new input. Transformer models are considered deep neural networks. The
reason for this is that transformers consist of multiple combined layers where
tokens get processed.
Vaswani et al. [4] introduced the transformer architecture in their famous
article ”Attention Is All You Need” in 2017. At this time, the best language
models were RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term
Memory). However, a big problem of those models was, that they processed

4

data sequentially. Vaswani et al. [4] argued that this problem ”precludes parallelization within training examples, which becomes critical at longer sequence
lengths”. The solution that was proposed in the paper was an architecture that
heavily relies on the so-called ”attention mechanism”.
2.1.1

The Attention Mechanism

In the context of transformer models, attention is basically a communication
mechanism. A token can ”communicate” with other tokens. The communication
happens, when the tokens are ”computed as a weighted sum of the values”
(Vaswani et al. [4]). This is possible, because every token holds information in
the form of a vector.

Figure 2: The scaled dot-product attention (left) and the multi-head attention
(right) mechanism proposed by Vaswani et al. [4].
A representation of the attention mechanism can be seen in Figure 2. The
”scaled dot-product attention” function proposed by Vaswani et al. [4] consists
of three inputs and one output. The inputs Q (query), K (key), and V (value)
are vectors, and they start from the same input data. First, Q and K are being
combined into one matrix by using matrix multiplication. In the next step, a
scaling operation is performed on the matrix. Vaswani et al. [4] suggest to use
√1 , where dk is the individual head size, to scale the matrix. The reason for
dk
this operation is to prepare the matrix for the softmax operation which comes
later. It helps to stabilize the input matrix variance. Next, a mask is added
to the matrix values. The mask has a lower triangular form and thus adds 0
to values that need to be kept and -∞ to values that are not kept. This step
is crucial in the transformer model ”to prevent leftward information flow” and
”to preserve the auto-regressive property” (Vaswani et al. [4]). The matrix is
then fed into a softmax function, and the result is a matrix that represents the
data dependent weights of the model. Lastly, the weights and the matrix V are
combined in a matrix multiplication.
As seen in Figure 2, the scaled dot-product attention is a component of the
”multi-head attention” mechanism. However, before the input values reach the
scaled dot-product attention function, they go through a linear transformation.
This process happens h times to run multiple heads in parallel, and as Vaswani
et al. [4] argue, it enhances the model’s performance. The last step is to
concatenate the results and to apply another linear transformation.
5

The transformer model proposed by Vaswani et al. [4] uses different types of
attention mechanisms. For example, the encoder block does not use the masking
operation in its attention function. It also utilizes a type of attention, called
”self-attention”. The self-attention approach only receives inputs that come
from the same source. On the other hand, the decoder block has a masked selfattention function and thereafter a ”cross-attention” function. Cross-attention
typically receives the inputs K and V from the encoder block and Q from the
decoder block.
After covering the implementation of the attention mechanism, I would like
to explain why it is implemented in the transformer model. The attention mechanism is one of the most important reasons for the success of the transformer
architecture. It is fundamental to transformer models and provides different
tokens with different weights. Therefore, the attention function computes similarity scores between tokens. We humans pay more attention to some parts
of a text than to others. So, the attention mechanism could also be seen as a
function that tries to mimic this behaviour. This is one of the big advantages
of transformer models, because it gives the transformer model the ability of
linking together different parts of a text. The possibility of understanding, or
rather mimicking to understand, the global context makes transformer models
usable for tasks such as language translation, text generation, question answering, and classification tasks. Another important aspect of transformer models is
the predefined information flow. The crucial point is, that future tokens cannot
interact with past tokens. Only the current tokens can interact with past tokens. This notion of time is necessary if one wants to use non-sequential models
like transformers for text generation.
2.1.2

Non-sequential Processes and Positional Encoding

RNNs and CNNs are sequential neural networks. They use recurrence and convolution to create sequential processes. As the name already suggests, sequential processes handle one data point after another. However, modern hardware
allows for the parallelization of tasks. Transformer models use this to their advantage by using non-sequential processes. Especially GPUs are very efficient
in carrying out tasks for transformer models. A big part of the computation
in transformer models is about matrix multiplication and the handling of tensors, where GPUs are very fast at. An important factor that creates problems
for sequential models are the path lengths for long-range dependencies. These
long-range dependencies often appear in NLP tasks, such as translation or text
generation. The specific problem is that a language models needs to remember these dependencies. Signals travel ”forward and backward [] to traverse in
the network” (Vaswani et al. [4]). Transformer models are usually better at
handling long-range dependencies because of the self-attention mechanism.
Transformer models do not use sequential processing, but this leads to the
problem that the model does not know the position of a particular token. To
solve this problem, Vaswani et al. [4] proposed positional encoding. The positional encoding should assign an absolute or relative position to each individual
token. This can be done, for example, through an embedding table or a dictionary. Vaswani et al. [4] have done this by using ”sine and cosine functions
of different frequencies”. In this approach ”each dimension of the positional
encoding corresponds to a sinusoid”. Vaswani et al. [4] found that both ap6

proaches yield similar results. After the positional encoding is performed, the
positional encodings are combined with the token embeddings by conducting a
simple addition. This can be done because both have the same dimensionality.

2.2

Quantization and LoRA

The most essential part of my chatbot application is the large language model
(LLM) that answers the users’ questions. There exist multiple approaches to use
and modify these LLMs. For example, the fine-tuning of a pre-trained model or
the usage of services that provide already existing models. In Section 3, I will
further explain which approach I exactly used for my project. One approach
I experimented with was the fine-tuning of Llama 2. Llama 2 is a pre-trained
LLM from the company Meta. The model is not entirely open source, but the
weights can easily be downloaded by anyone. The task of fine-tuning such a LLM
is rather challenging. The best performing version of this LLM is at the same
time also the version with the most parameters. This version is called ”Llama2-70b” and as the name already suggests, it has around 70 billion parameters.
Hu et al. [2] write in their paper, that the fine-tuning procedure ”updates all
the parameters of the pre-trained model”. During fine-tuning, many different
parameters need to be stored in RAM (random-access memory), such as the
model parameters, the gradients, and the optimizer states. Dettmers et al. [1]
claim that fine-tuning the older and smaller ”LLaMA-65B” model ”requires
more than 780 GB of GPU memory”. This would require a large amount of
modern GPUs. Additionally, it requires many hours to run, and thus the costs
for fine-tuning such a LLM can become very high. In this section I will present
two existing concepts for solving these problems.

Figure 3: Simplified representation of the LoRA mechanism. Adapted from Hu
et al. [2].
LoRA (Low-rank adaptation) is one such concept. It reduces the amount of
parameters that need to be trained and therefore also reduces the amount of
hardware required. Normally, such an approach would result in lower performance of the fine-tuned LLM. For example, so called ”catastrophic forgetting”
could set in. In this scenario a LLM would completely forget certain things it
7

had learned during pre-training. The LoRA technique does not have these kind
of problems. It is only fine-tuning a small separate amount of parameters and
does not interfere with the original LLM. These separate parameters are also
called ”adapters”. More specifically, in traditional fine-tuning the weights of the
original LLM are represented as a matrix W . During fine-tuning the weights
are changed and these changes are represented as ∆W . The fine-tuning result is
then W + ∆W . LoRA seeks to freeze the original weights W and instead of ∆W
it uses two lower rank matrices A and B, as can be seen in Figure 3. The theory
behind this is that LLMs ”can still learn efficiently despite a random projection
to a smaller subspace” (Hu et al. [2]). So, this big reduction in checkpoint size
leads to lower hardware requirements during the fine-tuning process, and it can
still preserve performance. But one might ask if there is any negative effect on
the inference process. This is not the case because W and BA can easily be
added together and result in a new updated weight matrix.
Quantization is another technique that can yield lower memory requirements
and result in smaller computational complexity. In the context of LLMs, data is
represented by floating-point numbers. The data type used in LLMs are 32-bit
floating point numbers. Quantization is the process of reducing the bit size,
for example, from 32-bit to 16 or 8-bit floating point numbers. This quantization can significantly lower the model’s size and increase its inference speed.
However, it also comes with a disadvantage, which is a lower accuracy. This is
very obvious, because when a data type gets reduced it automatically results
in loss of information. Therefore, the quantization step needs to take several
considerations into account and the trade-off between accuracy, inference speed
and memory savings needs to be done very carefully.
Dettmers et al. [1] proposed an approach to combine both Quantization
and LoRA, which they call QLoRA. The QLoRA technique is based on two
important pillars. The first one is called ”NormalFloat Quantization”. According to Dettmers et al. [1], NF quantization ”is an information-theoretically
optimal data type”. With this data type, all weights of the LLM need to be
converted into a range of [−1, 1]. QLoRA uses a 4-bit floating point numbers to
storage information. It then changes to 16-bit floating point numbers for computational purposes. The second pillar of QLoRA is ”Double Quantization”.
Double quantization is a ”process of quantizing the quantization constants for
additional memory savings” (Dettmers et al. [1]). In QLoRA the weights are
quantized in blocks. These blocks usually have a size of 64. During the process
of quantization, a memory overhead is created by the quantization constants.
Double quantization solves this problem by scaling the quantization constants.
This means that double quantization performs two quantization steps, hence
the name. From a theoretical perspective the QLoRA technique has many advantages. However, Dettmers et al. [1] also empirically proved that NF quantization improves performance compared to other data types, and that double
quantization makes memory usage more efficient.

2.3

Overview of Retrieval-Augmented Generation (RAG)

Before discussing the RAG concept, I would like to briefly explain the terms
”token” and ”embeddings”, as they are essential concepts to understand in the
field of NLP. A token refers to a subset of a text. One token can be as small
as one character or it could even be an entire word or sentence. Most modern
8

transformer language models divide words into subwords. For example, a word
such as ”Hello” could be divided into ”Hel” and ”lo”. This process is also called
”tokenization”. After that, tokens get converted into ”word embeddings”. An
embedding is a numerical representation of one token as a vector where v ∈ Rn .
This entire process is essential when preparing data for NLP.

Figure 4: Tokenization and embedding process.
Retrieval-augmented generation is an approach to combine LLMs with the
possibilities of similar search. LLMs are neural networks and they have shown
that they can actually store data in their weights. Lewis et al. [3] wrote in
their paper that LLMs can ”learn a substantial amount of in-depth knowledge
from data”. For example, the multimodal LLM called GPT-4 from OpenAI
was trained on a substantial amount of the internet. Although OpenAI also
used other techniques to improve the performance of their LLM, they mostly
used pre-training to equip it with knowledge and data. LLMs can perform
many different tasks and it seems like they always have the right answer for
every question. But even if they return information and answers to question,
it does not mean the answers are always correct. A common problem with
LLMs are ”hallucinations”. Hallucinations occur when a LLM does not have
the right factual information for the user’s question or prompt. In this case,
large language models often create an answer anyway. For people who are not
experts in a certain field, the answer could sound reasonable, even though it
is not factually correct. This is the main reason why users should be careful
while using chatbots. Hallucination mainly appears in LLMs when they have
not seen much data on the specific question or when they cannot draw any
connection between the question and the factually correct answer. Lewis et
al. [3] also mentioned in their paper that large language models ”cannot easily
expand or revise their memory” and that they cannot ”straightforwardly provide
insight into their predictions”. The authors also provide a suggestion to solve
these problems in their paper. They mention a hybrid architecture, where the
knowledge base is located outside of the LLM.
Lewis et al. [3] specifically refer to a ”retrieval-based” architecture. In this
case, the data could be stored in a vector database. A vector database is an
efficient method to store vector embeddings in a central database. Unstructured data (e.g. text or images) and structured data (e.g. data stored in SQL
databases or spreadsheets) can easily be converted into vector embeddings and
stored in a vector database. If the vector database is queried, it will return
the most similar result to the query vector. The process of searching the most
similar result is called ”similarity search”. Wang and Dong [5] present in their
review article multiple methods to calculate this similarity. In a vector database
the vectors are individual points in a multi-dimensional space. To calculate the
9

similarity, one can measure the distance between two points. As my thesis
project is about a chatbot, I will mostly write about text distance. According
to Wang and Dong [5], text distance ”describes the semantic proximity of two
text words from the perspective of distance”. In their paper they list the most
popular metrics to calculate the length distances. The best known are probably the Euclidean distance, the cosine distance and the Manhattan distance.
In Section 3.2 I will mention that I store larger text paragraphs in my vector
database. Especially for this use case, the cosine distance is more beneficial
than the other metrics (Wang and Dong [5]). The cosine distance does not
measure the distance, but it rather calculates ”the cosine of the angle between
two vectors”.
The RAG architecture strictly separates the text generation task from the
information storage and retrieval task. The former is taken over by the LLM and
the latter by the vector database. Lewis et al. [3] conducted an experiment and
compared the hybrid RAG approach with parametric models such as BART.
They showed that the RAG approach performs better than a simple LLM. In
particular, a Q&A (question and answer) chatbot can improve the accuracy
of its answer when using a RAG architecture. Besides this advantage, the
RAG method also has other benefits. For example, if one wants to expand the
knowledge base of a LLM, they will most likely perform fine-tuning. As already
discussed in Section 2.2, fine-tuning a LLM is a challenging task which costs a
significant amount of money and time. On the other hand, a vector database
can easily be expanded and filled with new information. If the RAG chatbot
does not return the correct information, it is easy to reproduce the error and
solve the problem. In case the data retrieved from the vector database is wrong,
some words could be removed, changed or added to the specific text. The new
vector embedding could help similarity search to find the correct information.
However, if the retrieved data is correct, but the LLM generates the wrong
answer, modifying the input prompt for the LLM could yield the right answer.

3

Technical Framework and Deployment Process

After covering the theoretical perspective, I would like to continue with an indepth explanation of my chatbot application. First, I will present a high-level
overview of my application, where I will explain the tasks of each individual
component. Then I will continue with information on the data collection and
processing step. The third point will be about cloud implementation and deployment of the chatbot application. Lastly, I will also briefly write about the
software testing methodology.

3.1

System Architecture

Figure 5 shows the architecture of my chatbot application. The application
itself can be accessed through the front-end interface, which is located on the
company’s ticketing system. Whenever the user submits a question to the application, it first reaches the back-end server. The back-end server is obviously the
core component of my chatbot application. It handles incoming and outgoing

10

Figure 5: RAG chatbot architecture.
API traffic, data retrieval and the entire application logic. The server first transmits the user input to the embedding model. As the name already suggests, the
embedding model converts the user input into a text embedding. In my application I use the embdding model ”text-embedding-ada-002” from Azure OpenAI
Service models. At the time of writing this thesis, the embedding model had an
output dimension size of 1536 and a maximum request token size of 8191. The
embedding model will then return a vector embedding. The server redirects
this vector embedding to the vector database. In the current version of this
application, I use the Qdrant vector database. Qdrant is an open source vector
database that is fast and easily scalable on the cloud. The database has a filter
function to pre-filter for specific payloads or an id. In my application, I created
a filter for the different machine types. This allows the similarity search algorithm to ignore many vectors and only focus on the relevant ones. The vector
database retrieves the three paragraphs of text most similar to the user’s input.
The server retrieves three different text embeddings from the database because
this amount provides the LLM with enough context. Only one text embedding
might not contain all the necessary information to correctly answer the user’s
question. When retrieving five or more text embeddings, the LLM could confuse
necessary information with unnecessary information. Through trial and error,
I found that this is generally true for my chatbot application. When the server
receives the results from the vector database similarity search, it sends the user
input, the context data and a prompt to the LLM. A prompt can be used to
provide the LLM with additional information. For example, in which use case
the chatbot application is being used in, how it should format the response,
and who the end user for the chatbot application is. The current version of my
generative AI chatbot uses the ”gpt-35-turbo” LLM from Azure OpenAI Service
models. The model has a maximum request token size of 4096. After processing
the information, the LLM returns the generated text in a streamed response.
This means that every token gets transmitted individually to the server and
thus also to the front-end interface. The user can interact multiple times with
the chatbot application. The chat history is stored in the application session.
Therefore, the user can perform follow-up questions. If the user does receive
an answer that deviates slightly from the correct answer, then the user can ask
more precisely. Every time the user sends an input, the same process starts
over again. The only difference is that the chatbot knows the chat history of
the current session. For every new input, the server conducts a similarity search

11

in the vector database. That is due to the fact that the question may change
or the user may provide additional context.
The back-end server was written in Python. I used the Flask framework,
which is a web framework for Python. The framework simplifies the work by
handling sessions, templates and asynchronous requests. The communication
between the front-end interface and the back-end server is carried out by Gunicorn. Gunicorn is a WSGI (Web Server Gateway Interface) server for the
production environment. Gunicorn ensures that the application runs reliably.
In case there is much traffic directed to the server, Gunicorn can handle the
requests through multiple asynchronous processes. Normally, a production environment also includes a reverse proxy server. I chose Nginx for my chatbot application. Nginx fulfils multiple tasks, such as load balancing, caching
and several security related tasks. The front-end was written in HTML, CSS
and Javascript. The chatbot interface can be accessed by company employees
through the ticketing software. It is a HTML iframe that can be opened by
clicking on a button. If the user accesses the chatbot from an individual ticket,
the application automatically takes the machine type from the ticket.

3.2

Data Collection and Processing

During the development of my application, I got access to multiple sources of
data. The first data source that I took into consideration was the ticketing data.
The customer service department has an internal software to handle customer
problems. The customer can open a ticket inside the software and an employee
of the customer service department enters into a conversation with the customer.
The employees of the customer service department are mostly technicians, who
are experts in a certain type of printer. Therefore, the conversation between
the employee and customer in a ticket may contain valuable information for my
generative AI chatbot. In addition, I was allowed to download the data in form
of a CSV (Comma-separated values) file from the internal ticketing software. I
created a Python script which converted the CSV file into a Pandas DataFrame.
This allowed me to filter the data and remove unnecessary characters such as
line breaks. However, there were several problems with this data. For example,
tickets were often not closed properly, which means that it was unclear whether
the employee’s suggestions really worked. Customers also sometimes send pictures of the technical problem without any text explanation of the problem.
Most customers communicated in English, but some customers also wrote in
other languages, which added to the complexity. Ultimately, this meant that
someone had to manually check and modify every single ticket. Doing this for
several thousand tickets was not feasible, so in the end I did not use the data
from the ticketing software.
Another source of data was technical documentation in the form of PDF
files. The company has several types of this documentation, for example, machine manuals, diagnostic files and maintenance files. Unlike the ticketing data,
technical documentation is precise, relatively easy to understand and consistent.
This meant that the data was well suited for a knowledge database. The different documentation files mostly had the same structure, which also made it easier
to automate the data processing task. Extracting data from PDF files is rather
difficult, so I used the Python package called PDFMiner. I only extracted text
data and split it into the corresponding subsections. I then uploaded the data
12

into the vector database, which acted as the knowledge base for this chatbot
application. The text paragraphs inside the technical documentation ranged
from simple explanations to the solutions for the most frequent problems. In
the end, the vector database contained several hundred text embeddings that
covered a wide range of problems.
The last source of data which I used were the so-called ”gold answers”. Gold
answers are a combination of the most common questions and the correct answer
to the specific question. This list of 25 questions and answers was created by
a domain expert. The data represents a ground truth, which is very important
because the company’s products are large and complicated printers. Therefore,
it is crucial that my chatbot application provides customers with the right suggestions. I did not add the golden answers to the chatbot’s knowledge base, as
there was already enough data. Instead, I decided to use it as test data. I will
explain this in more detail in Section 4.
The data was stored in a vector database. There are also other databases,
such as ”graph databases”. However, all of the data is in text format (unstructured data), which vector databases can handle better and faster.

3.3

Cloud Implementation and Deployment

The implementation and deployment phases are critical for finalizing a software
application. It is the transition from the development and testing cycle to the
actual usage in a production environment. To achieve this, a software engineer
must setup all databases correctly, ensure that the individual components work
as intended and deploy the application on a server or in the cloud. The containerization of a software application can be very advantageous. For example,
containerized applications allow software developers to create an applications
without considering the deployment environment. The only thing that needs
to be set up are the environment variables. This minimal configuration of a
container simplifies and accelerates the whole implementation and deployment
process. I used Docker as the containerization software for my chatbot application. Docker is a free software that works on the OS (operating system)
virtualization level. The first step with Docker is to create a Dockerfile. This is
a file that contains several commands to assemble a Docker image. Normally, a
Dockerfile has commands to specify the runtime for a certain programming language, commands to copy the content directory and commands to expose one or
more ports. The Docker image assembled from these commands is a lightweight
executable package, that contains everything to run the software application.
The Docker image can be distributed to a server or cloud platform. The Docker
image can be executed on the selected platform. This creates a Docker container, which is a runtime instance of a Docker image. The container is an
independent and isolated environment. A single machine OS kernel can handle
many containers. In contrast to containers, a VM (virtual machine) takes up
more disk space and RAM (random-access memory). A reason for this is that
each VM needs an entire operating system to run. Although my chatbot project
is just a proof of concept and just a limited number of employees will use it, I
still thought it was reasonable to deploy the application to the cloud. I uploaded
the Docker image to the cloud and created a running Docker container. The
combination of cloud computing and a containerized software application is very
powerful. It allows the software application to be dynamically scaled according
13

to traffic requirements. Currently, the implementation and deployment process
to the cloud is still being conducted manually.

3.4

Software Development and Testing Methodology

I used the ”agile” software development methodology for my chatbot project.
During my internship I did not work in the software solutions department, but
in the customer service department. The reason was that I could understand
how the costumer service department worked. I spoke directly to the employees
and asked for their input, so that I could build a chatbot that would meet their
needs. Therefore, I was the only person who wrote software and who used the
agile methodology. Although the agile methodology is mostly used to improve
the development work in a cross-functional team environment, I nevertheless
used some aspects of the methodology. One technique I used is called ”backlog”.
The backlog is a list of user stories that need to be implemented. User stories
are simple descriptions for features that were communicated by stakeholders.
This list helped me to organize my ideas into concrete tasks and to prioritize
them accordingly. I also held a meeting once a month, which one could call a
”sprint review meeting”. During the meeting I presented the work that I had
completed to this point to several stakeholders. Additionally, I discussed with
the stakeholders what could be improved and what the next steps should be.
For the final testing of my software code, I mainly relied on ”non-functional”
testing. This is a type of testing that tests the system as a whole, rather than a
single function or component. For example, I tested how the software application
reacts to stress. I created a small Python script which pushed the application to
its boundaries. As already explained in other sections, the generative AI chatbot
will only be used by a small limited number of employees. Nevertheless, I wanted
to know how the software application would respond to 10, 20 or even more
simultaneous user requests. A higher amount of concurrent requests resulted in
a small, negligible slowdown. The asynchronous functions and multi-threaded
architecture worked as intended. Some employees also conducted a small scale
usability test. The results showed that there were no usability issues, the chatbot
was easy to use and the application behaved correctly.

4

Evaluation of Results

I conducted two types of evaluation to assess the quality of the chatbot’s responses. The first type of evaluation was an automated Python script, where a
LLM acts as a judge. I used the ”evaluation” module from LangChain’s framework to perform the task. During the evaluation process, the script first asks
my chatbot application a question from the ”gold answers” list. As already explained in Section 3.2, this list represents a ground truth. For that reason, the
answer column from this list is used as a reference. The LLM then compares the
answer from my chatbot application with the relevant reference. Additionally,
the script instructs the LLM to rate the accuracy of the chatbot’s response on
a scale of 1 to 5. The scoring labels are the following:
• Score 1: The answer is not related to the reference.
• Score 2: The answer is slightly related to the reference.
14

• Score 3: The answer has some relevance, but it has factual inaccuracies.
• Score 4: The answer is very close to the reference, but it has small inaccuracies.
• Score 5: The answer is perfectly accurate and aligned with the reference.
When the evaluation process is finished, the script returns a list of scores and
a nuanced explanation for each rating. After running the script, the following
scores returned:
Score label
Score 1
Score 2
Score 3
Score 4
Score 5

Accumulated score
0
0
8
13
4

Table 1: Automated LLM evaluation results.
As one can see in Table 1, the responses from my generative AI chatbot
always had some relevance. However, 8 out of 25 answer (32%) had some factual
inaccuracies. More than 50% of the anwers were very close to the ground truth.
The remaining 12% of responses were perfectly accurate, according to the LLM
judge.
The second type of evaluation was a manual evaluation conducted by a domain expert. In contrast to the LLM judge, the human judge had the possibility
to ask follow-up questions. The results of this evaluation were communicated
verbally. The feedback from the domain expert was, that most of the chatbot’s
responses are very accurate and had a high quality of information. Nevertheless, the domain expert also said, that the chatbot does not correctly respond
to some questions.

5

Discussion

As mentioned in Section 4, the automated LLM judge claimed that the quality
of the chatbot’s responses always had some relevance. At the other hand, the
domain expert rated the chatbot more positively than the LLM judge. An
explanation for this difference could be that the LLM judge did not have the
possibility of asking follow-up questions. The human expert was able to analyse
the chatbot’s response and ask further questions to the system if the answer was
not satisfactory or did not contain the necessary information. This allows the
system to perform a new similarity search on the knowledge base and to correct
its error.
As already discussed in Section 2.2, fine-tuning a local LLM would be an
alternative to the RAG architecture. However, it requires much more computing
power and time. I experimented with the fine-tuning approach for several weeks,
but at the end we decided that the RAG architecture is more favorable for our
use case.

15

6

Conclusion and Future Work

Generally speaking, I would argue that the objective of this thesis internship
was achieved. I have built a chatbot that can correctly answer nuanced technical
questions on a consistent basis. Additionally, this RAG chatbot is a cost- and
time-saving solution. This project was only a proof of concept, so there are still
many aspects of the chatbot that could be improved. The first improvement
could be a CI/CD (continuous integration/continuous deployment) pipeline.
This is currently done manually, but an automated pipeline could accelerate
the implementation and deployment process. Another important improvement
would be the use of a distributed version control system such as Git. This
would allow versioning and collaboration on coding the chatbot. A combination
of fine-tuning a LLM on the cloud and a RAG architecture could also improve
the quality of the chatbot’s responses. Future possibilities could also include an
expansion of the user base. By this, I mean that the chatbot should not only
be used internally but should also be made available to customers.
I encountered numerous challenges during the development and implementation phase of the project. But through independent research, with the help
of my thesis supervisor or company employees, I was able to solve all problems.
I learnt a lot of new things during the internship, such as how transformer
models work, how to fine-tune a LLM and how embedding models work. I was
also capable to expand my knowledge of Python, Pytorch, Docker and other
technologies.

Acknowledgement
I would like to thank my thesis supervisor, Prof. Dr. Oswald Lanz, for granting
me the opportunity to work on this project and for his support during the
project. Additionally, I would like to thank Durst Group AG for providing me
with the chance to conduct my thesis internship at their company.

References
[1] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint
arXiv:2305.14314, 2023.
[2] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
large language models. arXiv preprint arXiv:2106.09685, 2021.
[3] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive
nlp tasks. Advances in Neural Information Processing Systems, 33:9459–
9474, 2020.
[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advances in neural information processing systems, 30, 2017.
16

[5] Jiapeng Wang and Yihong Dong. Measurement of text similarity: a survey.
Information, 11(9):421, 2020.

A

Code snippets

Figure 6: Code snippet for data processing.

Figure 7: Code snippet for uploading data to the vector database.

17

Figure 8: Code snippet for server API call.

Figure 9: Code snippet for LLM text generation.
Figure 6 shows a small portion of software code that was used to process
PDF data. The function runs through all items of the tree and extracts the
text. Figure 7 is a code snippet for populating the vector database with data.
It creates a text embedding from a text section and uploads the id, vector and
payload to the database. Figure 8 demonstrates a small piece of Javascript code
from the user interface. This code tries to create a connection to the server and
fetches the streaming response. Lastly, in Figure 9, the server retrieves data
from the vector database. It then sends the retrieved data, a prompt and the
user query to the LLM.

18

