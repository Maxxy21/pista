\chapter{Introduction}

\label{ch:introduction}

Communicating business ideas effectively can determine the success or failure of a startup. The startup pitch, as a primary format for communicating visions, business models, and potential to investors and stakeholders, is traditionally evaluated through in-person presentations and expert judgment. Research shows that investment decisions depend heavily on the quality of pitch presentations, which play a major role in securing financial support\cite{masterpresentat}. Many business functions have changed through technology. Traditional pitch evaluation has not evolved much and still relies on in-person presentations and individual expert judgment.



In recent years, GenAI, particularly large language models, has become a useful tool for analysis. These models handle language, context, and structure well across many domains \cite{Ozince2024}. GPT-4 represents the latest stage in this line and performs at levels similar to humans on complex analytic tasks. This suggests clear potential to improve pitch evaluation processes through new approaches \cite{gpt}.



However, pitch evaluation in the startup community faces several obstacles. Many processes use inconsistent standards. Quality feedback is hard to access and evaluations take too long \cite{StartupEvaluati, Kalvapalle2024}. Current methods require significant time for each pitch and often yield very different interpretations among evaluators. These issues are common for entrepreneurs from emerging markets and other underrepresented groups, who have limited access to expert feedback networks \cite{BreakingBarrier}.



This thesis develops Pista, a GenAI-based pitch evaluation system, and compares it with Winds2Ventures (W2V), a research partner’s startup evaluation system used in practice. The system processes pitches in multiple formats such as text, files, and audio\footnote{Project Website, \url{https://pista-app.vercel.app}}. It provides feedback across four dimensions: problem-solution fit, business model viability, team execution capability, and pitch quality. The GenAI system focuses on textual content analysis and aims to provide fast and consistent evaluation. The comparison explores where GenAI evaluation works well and where traditional methods may be more suitable \cite{TheFutureofAIEv}.

\subsection{Motivation}
Traditional pitch evaluation depends on expert judgment and live presentations. This limits access and creates variability across evaluators. Founders outside major hubs often lack timely, structured feedback. A GenAI system that applies a clear rubric can provide consistent guidance at low cost and support early iteration.

\subsection{Objectives}
This thesis aims to design and implement a practical evaluation tool for startup pitches and to compare it with a practitioner platform. The objectives are to deliver a working system, apply a transparent four-dimension rubric, and study how scores differ from a partner platform used in practice.

\subsection{Approach}
We implement Pista using Next.js, Clerk, and Convex, with GPT\,4 for evaluation. The system accepts text or audio, generates follow-up questions, and scores the pitch once answers are provided. We then compare Pista’s results with W2V on the same pitches to analyze dispersion, bias, and dimensional differences.

\subsection{Structure of the thesis}
Chapter~\ref{ch:soa} reviews related work. Chapter~\ref{ch:problem-solution} describes the system. Chapter~\ref{ch:evaluation} presents the comparative evaluation and discusses findings. The conclusion summarizes implications and future work.

\subsection{Contributions}
\begin{itemize}
  \item A working GenAI-based evaluation system (Pista) with a four-dimension rubric and Q\&A step before scoring.
  \item An implementation aligned with a simple, reproducible stack (Next.js, Clerk, Convex, GPT-4) suitable for study.
  \item A comparative evaluation setup against a practitioner system (W2V) to analyze scoring patterns and bias.
\end{itemize}

\subsection{Research Questions}
\begin{itemize}
  \item RQ1: How do Pista’s scores compare with W2V assessments across identical pitches?
  \item RQ2: Does prompt calibration reduce score convergence and improve discrimination?
  \item RQ3: Which evaluation dimensions show the largest differences between GenAI and practitioner scoring?
\end{itemize}

