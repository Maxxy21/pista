% chapters/4-evaluation.tex
\chapter{Evaluation and Results}
\label{ch:evaluation}

This chapter compares the Pista system with an existing AI-powered evaluation platform called Winds2Ventures (W2V)\footnote{\url{https://w2v.network/}}. The research addresses a key question: how do different AI evaluation systems compare when rating the same startup pitches, and what can we learn from their agreement patterns?

To answer this question, both AI systems were compared on how they scored the same startup pitches. The analysis examined whether the two systems agreed on which pitches were good, average, or below average. The comparison focuses on the final overall scores that each system produces. Statistical measures of agreement were analyzed to understand how well different AI evaluation approaches align with each other.

Twenty-two startup pitches from university competitions were analyzed using statistical methods to measure agreement between the systems. The results show that the systems agree moderately well, but there are some systematic differences in their evaluation approaches. Pista tends to give slightly higher scores than W2V, which appears to use more conservative evaluation criteria. The Cohen's kappa coefficient shows moderate agreement at 0.450, indicating the systems align much better than random chance.

This comparison helps us understand how different AI evaluation approaches perform and what we can learn from comparing automated assessment systems. The findings suggest that different AI evaluation systems can have distinct characteristics and evaluation patterns, even when assessing the same content. The results provide insights into the variety that exists within automated evaluation systems and how different algorithmic approaches can lead to different assessment outcomes. These findings are important for understanding the current diversity and future potential of AI-powered evaluation platforms.

\section{Methodology}
\label{sec:methodology}

This section explains how the Pista system was tested against the Winds2Ventures platform. The final version of Pista was used, which uses GPT-4 with specialized prompts to evaluate startup pitches consistently. The testing approach focuses on comparing final scores from both systems using the same set of university competition pitches. Statistical methods were selected that account for chance agreement to provide meaningful comparisons.

\subsection{Dataset Selection}
\label{subsec:dataset}

Twenty-two startup pitches from university competitions were used as the test dataset for this evaluation. University pitches were chosen because they all follow the same format and contain similar information, which makes it easier to compare how the two systems evaluate them. University competition pitches provide a standardized structure that ensures both evaluation systems receive comparable input data. The pitches represent real student ventures that have been through competitive selection processes.

The pitches cover different areas like technology, healthcare, agriculture, and consumer services. This variety helps me see how both systems handle different types of businesses across multiple industry sectors. The diversity ensures that the evaluation results reflect how the systems perform across different business models and market contexts. Each industry sector presents unique challenges and opportunities that test different aspects of the evaluation frameworks.

Each pitch includes the standard parts you expect: the problem they're solving, their solution, market analysis, team information, and how they plan to make money. Since all pitches follow the same format, both evaluation systems get the same type of information to work with.

The W2V evaluation results for these pitches were provided by the thesis supervisor to enable this comparative analysis. This supervisor-facilitated comparison allows for meaningful statistical analysis between different AI evaluation approaches using standardized pitch data.

\subsection{Evaluation Frameworks}
\label{subsec:frameworks}

Both systems need to be explained since they use different ways to evaluate pitches. Understanding these differences helps explain why the scores sometimes vary between the systems. The frameworks use different criteria and weighting approaches, which can lead to different assessments of the same pitch. Comparing these frameworks helps identify the strengths and limitations of each evaluation approach.

\subsubsection{Pista Evaluation Framework}

Pista uses four main areas to evaluate pitches, with different weights for each:

\begin{enumerate}
    \item \textbf{Problem-Solution Fit (30\%)}: How well the startup understands the problem and how good their solution is
    \item \textbf{Business Model \& Market (30\%)}: How they plan to make money and if the market is big enough
    \item \textbf{Team \& Execution (25\%)}: How capable the team is and if they can actually build their solution
    \item \textbf{Pitch Quality (15\%)}: How well they present their idea and communicate their vision
\end{enumerate}

Each area gets a score from 1 to 10, and then Pista calculates a final score using the weights above. The system also provides written feedback to explain the scores and reasoning behind each assessment. The weighted scoring approach ensures that the most important aspects of a startup pitch have greater influence on the final evaluation. This systematic approach helps maintain consistency across different pitches and evaluation sessions.

\subsubsection{Winds2Ventures Evaluation Framework}

W2V uses nine different criteria in its AI-powered evaluation system, focusing on aspects like problem clarity, solution viability, market analysis, team quality, and business model validation. The platform uses automated analysis trained on investment patterns and business assessment methodologies. The system appears to incorporate conservative evaluation approaches that reflect real-world investment considerations and startup challenges.

The key difference is that W2V uses a different algorithmic approach compared to Pista's evaluation methodology. This system tends to be more conservative in its assessments, applying stricter evaluation criteria when rating pitch quality. The evaluation algorithm appears to weight risk factors and practical business challenges more heavily in its assessment process. The system's conservative approach reflects evaluation patterns that prioritize cautious assessment of startup viability and market potential.

\subsection{Comparative Analysis Approach}
\label{subsec:methodology-approach}

Since both systems work differently, a way to compare them fairly was needed. The decision was made to focus on the final overall scores that each system produces as the primary basis for comparison. This approach allows the systems' ultimate assessments to be compared regardless of their different internal processes. The comparison method focuses on whether both systems reach similar conclusions about pitch quality.

Both systems give final scores on a 1-10 scale, which makes it possible to compare them directly. Pista creates its final score by combining its four area scores with weights, while W2V gives an overall "Investibility" rating based on its nine criteria. Even though they calculate scores differently, the common 1-10 scale lets me compare how well they agree.

\section{Results and Analysis}
\label{sec:inter-rater-reliability}

This section shows the statistical analysis of how well Pista and W2V agree with each other. Cohen's kappa coefficient was used as the main statistical measure, which is better than just looking at percentages because it accounts for agreements that happen by chance.

The scores were grouped into three levels to make the comparison easier: Below Average (1-4), Average (5-7), and Good (8-10). This helps show whether both systems consistently identify which pitches are strong, average, or weak.

The following table shows the complete statistical analysis of the 22 pitches:

\begin{table}[h]
\centering
\caption{Cohen's Kappa Analysis: Pista vs Winds2Ventures}
\label{tab:cohens-kappa}
\begin{tabular}{lr}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Cohen's Kappa & 0.450 \\
Interpretation & Moderate Agreement \\
Observed Agreement & 63.6\% (14/22 pitches) \\
Mean Score Difference & 0.17 (Pista higher) \\
\hline
\textbf{Pista Scores} & \\
Mean & 5.36 \\
Standard Deviation & 0.70 \\
\hline
\textbf{W2V Scores} & \\
Mean & 5.20 \\
Standard Deviation & 0.72 \\
\hline
\textbf{Category Distribution} & \\
Pista Below Average (1-4) & 6 pitches \\
Pista Average (5-7) & 10 pitches \\
Pista Good (8-10) & 6 pitches \\
W2V Below Average (1-4) & 9 pitches \\
W2V Average (5-7) & 8 pitches \\
W2V Good (8-10) & 5 pitches \\
\hline
\end{tabular}
\end{table}

\subsection{Statistical Interpretation}

The Cohen's kappa coefficient of 0.450 shows moderate agreement between Pista and W2V according to standard statistical interpretation guidelines. This result means the systems agree much more than they would by random chance, but there are still some systematic differences in how they evaluate pitches. The moderate agreement level suggests that both systems capture similar underlying patterns in pitch quality. However, the differences indicate that each system brings unique perspectives to the evaluation process.

The systems agreed on 63.6\% of the pitches (14 out of 22), meaning both systems put the pitch in the same category (below average, average, or good). This suggests that different AI evaluation systems can identify similar patterns in pitch quality assessment about two-thirds of the time.

The analysis shows that Pista gives slightly higher scores on average (0.17 points higher). This pattern suggests that different AI evaluation systems can have distinct scoring tendencies, with some being more optimistic while others apply more conservative assessment criteria.

\subsection{Score Distribution Analysis}

Looking at how the systems categorized the pitches shows clear differences in their evaluation tendencies. W2V rated more pitches as "Below Average" (9 pitches) compared to Pista (6 pitches), while Pista rated more pitches as "Good" (6 pitches) compared to W2V (5 pitches). This pattern suggests that W2V applies stricter evaluation criteria when assessing pitch quality. The differences reflect different algorithmic approaches to weighing risk factors and business viability in the evaluation process.

This pattern confirms that W2V uses more conservative evaluation criteria, which appears to be built into its assessment algorithm. The system seems designed to apply stricter standards when evaluating startup potential, possibly incorporating more cautious approaches to risk assessment and business viability analysis.

Both systems put most pitches in the "Average" category, which is what you'd expect for university competition participants. These student ventures usually have good ideas and potential, but they still need more development before they're ready for investment.

\subsection{Discussion and Implications}

These results show that different AI evaluation systems can produce meaningful assessments of startup pitches, with each system bringing distinct evaluation characteristics. The moderate agreement level demonstrates that AI systems can capture many important patterns in pitch evaluation consistently. However, the systematic differences suggest that different AI evaluation approaches can have unique characteristics and assessment tendencies. The findings highlight the diversity that exists within AI-powered evaluation systems and how different algorithmic approaches can complement each other.

The moderate agreement shows that Pista captures many of the same patterns that other AI evaluation systems identify when assessing startup pitches. This makes the system valuable for situations where you need to evaluate many pitches consistently, like initial screening for competitions or providing standardized feedback to entrepreneurs. The AI system can process large volumes of pitches quickly while maintaining consistent evaluation criteria. These capabilities make AI evaluation particularly useful for high-volume assessment scenarios.

However, the different scoring tendencies between AI systems suggests that using multiple evaluation approaches can provide more comprehensive assessment perspectives. Different AI systems may weight various factors differently, leading to more balanced overall evaluations. The combination of different algorithmic approaches can help identify both optimistic and conservative viewpoints in startup assessment. Understanding these differences helps users interpret AI evaluation results more effectively.

The best approach may be to use multiple AI evaluation systems to get different perspectives on startup potential, combining their insights to create more comprehensive assessments. This multi-system approach gets the benefits of AI efficiency and consistency while capturing different evaluation methodologies and assessment criteria. Understanding how different AI systems approach evaluation helps users make more informed decisions about startup potential.
