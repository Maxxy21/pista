% chapters/4-evaluation.tex
\chapter{Evaluation and Results}
\label{ch:evaluation}

This chapter evaluates the Pista system by comparing it with Winds2Ventures (W2V)\footnote{\url{https://w2v.network/}}, the partner evaluation system introduced in Chapter~\ref{ch:introduction}. The comparison examines differences in scoring patterns and evaluation approaches between the GenAI-powered system and the partner platform.

Evaluating startup pitches involves subjective judgment and domain expertise. To validate the Pista system, comparison with an existing evaluation platform provides insights into how GenAI approaches differ from human-driven assessment methods.

The comparison examined differences in scoring patterns, evaluation consistency, and assessment approaches between the two systems. The analysis focused on understanding where each system shows strengths and limitations.

Comparing GenAI evaluation with existing platforms helps understand the practical utility and limitations of automated assessment tools for startup pitch evaluation.

The comparison addressed three key questions:
\begin{enumerate}
    \item How do Pista scores compare with W2V assessments?
    \item What patterns emerge in the scoring differences?
    \item What are the practical implications for using GenAI evaluation tools?
\end{enumerate}

\section{Methodology and Experimental Design}
\label{sec:methodology}

All results in this chapter use the calibrated scoring prompt (\texttt{PROMPT\_VERSION = criteria\textrm{-}v1.2}) with rubric anchors, evidence gating, aspect first scoring, and a lower sampling temperature for scoring. When Q\&A is enabled, the question generator asks for 1--3 structured, evidence seeking questions and accepts JSON or a numbered list fallback.

\subsection{Metrics}
To evaluate distribution quality and alignment, the analysis reports:
\begin{itemize}
  \item \textbf{Score concentration}: share of exact 7.0 scores before/after calibration
  \item \textbf{Dispersion}: variance or IQR per criterion
  \item \textbf{Rank alignment}: Kendall's $\tau$ with baseline/commercial assessments
  \item \textbf{Evidence sensitivity}: proportion of scores $\leq 4$ when specific evidence is missing
\end{itemize}

\section{Future Work: Ensemble Evaluation}
The current implementation uses a single provider (GPT\,4) with rubric anchored prompts. A practical extension is an ensemble across multiple models to improve stability and evidence coverage:
\begin{itemize}
  \item \textbf{Providers}: Add adapters for OpenAI, Anthropic, and Google; run all providers per criterion in parallel.
  \item \textbf{Aggregation Policy}: Aggregate aspect scores via trimmed mean (e.g., 20\% trim if $\geq$3 providers), then average to the criterion score; record provenance.\footnote{Bump \texttt{POLICY\_VERSION} to \texttt{scoring-policy-v3}.}
  \item \textbf{Disagreement Flags}: If interquartile range across provider scores exceeds a threshold, mark the criterion as low confidence.
  \item \textbf{Adjudication (Optional)}: For flagged criteria, use an adjudicator prompt to merge rationales into the final JSON.
  \item \textbf{Disagreement-Driven Q\&A}: When flagged, generate 1--3 targeted questions for the highest-variance aspects and re-score on answers.
  \item \textbf{Reporting}: Compare ensemble vs. single provider on score dispersion and Kendall's $\tau$ against the baseline (W2V), and document cost/latency trade-offs.
\end{itemize}

\subsection{Dataset Selection and Characteristics}
\label{subsec:dataset}

Twenty-two startup pitches from university entrepreneurship competitions were evaluated by both systems. University competition pitches provided consistent content and format, enabling direct comparison between the two evaluation approaches.

Both systems evaluated all 22 pitches, covering diverse sectors including technology, healthcare, agriculture, and consumer services.

The pitches were obtained through university entrepreneurship programs with appropriate permissions for research use.

Each pitch contained the fundamental components required for comprehensive startup evaluation: clearly defined problem statements, proposed solution descriptions, market opportunity analysis, team capability presentations, and business model articulation. The standardized competition format ensured consistent information availability across all evaluated pitches, facilitating meaningful cross-system comparison.

\subsection{Evaluation Framework Comparison}
\label{subsec:frameworks}

Fundamental differences in evaluation architecture between the two systems necessitated careful methodological consideration for comparative analysis. Each platform employs distinct assessment criteria and weighting mechanisms that reflect different philosophical approaches to startup evaluation.

\subsubsection{Pista Evaluation Framework}

The Pista system employs a four-dimensional weighted assessment structure designed to capture critical aspects of startup viability. The dimensional weighting was calibrated based on venture capital research findings and established industry evaluation practices.

The assessment framework comprises four weighted dimensions:
\begin{enumerate}
    \item \textbf{Problem-Solution Fit (30\% weight)}: Examination of problem definition clarity, solution innovation depth, market understanding sophistication, competitive differentiation strength, and value proposition coherence.

    \item \textbf{Business Model \& Market (30\% weight)}: Analysis of revenue model sustainability, market size estimation credibility, go-to-market strategy feasibility, customer acquisition viability, and scalability potential.

    \item \textbf{Team \& Execution (25\% weight)}: Assessment of team competency profiles, domain expertise relevance, historical performance indicators, resource allocation efficiency, and implementation strategy depth.

    \item \textbf{Pitch Quality (15\% weight)}: Evaluation of presentation structure, evidence integration effectiveness, narrative coherence, and persuasive communication quality.
\end{enumerate}

Quantitative scores are generated on a standardized 1-10 scale for each dimension, accompanied by qualitative feedback containing specific improvement recommendations. The final assessment emerges through weighted aggregation of dimensional scores.

\subsubsection{Winds2Ventures Evaluation Framework}

The platform operates through a nine-criteria assessment methodology focused on investment readiness and commercial viability indicators. Each evaluation criterion contributes to an aggregate "Investibility" rating that reflects overall investment attractiveness from a professional investor perspective.

The evaluation criteria encompass:
\begin{enumerate}
    \item \textbf{Problem Clarity}: Assessment of problem definition precision and market relevance
    \item \textbf{Solution Viability}: Analysis of proposed solution feasibility and effectiveness potential
    \item \textbf{Market Size Estimation}: Evaluation of addressable market opportunity and growth projections
    \item \textbf{Competitive Advantages}: Examination of differentiation strategies and sustainable positioning
    \item \textbf{Team Assessment}: Analysis of team competencies, experience, and execution capabilities
    \item \textbf{Business Model Viability}: Investigation of revenue mechanisms and sustainability factors
    \item \textbf{Go-To-Market Strategy}: Review of market entry approaches and scaling methodologies
    \item \textbf{Competitive Landscape}: Assessment of market dynamics and competitive positioning
    \item \textbf{Funding Ask and Use}: Analysis of capital requirements and allocation strategies
\end{enumerate}

The W2V evaluation process involves experienced investors and business professionals who contribute extensive industry knowledge and practical investment experience. This human expertise emphasizes realistic market dynamics and execution risk factors, typically resulting in more conservative assessment patterns than algorithmic evaluation approaches.

\subsection{Score Mapping and Comparison Methodology}
\label{subsec:methodology-approach}

The divergent evaluation frameworks presented methodological challenges for direct comparative analysis. To address these complexities, a conservative analytical approach was adopted that focused primarily on overall assessment scores rather than attempting complex dimensional mappings between disparate criteria sets.

The comparison methodology centered on Pista's weighted overall scores against W2V's Investibility ratings. Both systems generate standardized 1-10 scale assessments, facilitating direct numerical comparison without requiring complex score transformations or normalization procedures. This approach preserved the integrity of each system's underlying evaluation philosophy while enabling meaningful quantitative analysis.

\section{Results and Analysis}
\label{sec:results}

\subsection{Overall Performance Patterns}
\label{subsec:performance}

The comparative analysis revealed systematic differences between evaluation systems that extend beyond random variation, suggesting fundamental divergences in assessment philosophy and methodology. These patterns emerged consistently across all evaluated pitches, indicating structural rather than circumstantial differences.

Across all 22 evaluated pitches, Pista consistently generated higher scores than W2V, with no exceptions observed. The average scoring differential measured +1.70 points, representing a substantial and systematic bias that suggests underlying philosophical differences in evaluation approach rather than random measurement variation.

\begin{table}[ht]
    \centering
    \caption{Comprehensive Score Comparison Analysis}
    \label{tab:score-comparison}
    \begin{tabular}{lccc}
        \toprule
        \textbf{System} & \textbf{Average Score} & \textbf{Score Range} & \textbf{Standard Deviation} \\
        \midrule
        Pista & 6.81/10 & 6.0-8.0 & 0.50 \\
        Winds2Ventures & 5.11/10 & 4.3-6.4 & 0.63 \\
        \midrule
        \textbf{Difference} & +1.70 & - & - \\
        \bottomrule
    \end{tabular}
\end{table}

Analysis of scoring distributions revealed significant differences in discrimination capabilities between the evaluation systems.

Pista scores demonstrated clustering within a relatively narrow 6.0-8.0 range, producing a standard deviation of 0.50. In contrast, W2V scores exhibited broader distribution spanning 4.3-6.4 points with higher variance ( = 0.63). These distribution patterns suggest different approaches to quality discrimination, with W2V demonstrating greater sensitivity to quality variations across evaluated pitches.

\subsection{Systematic Bias Identification}
\label{subsec:bias}

Detailed pattern analysis exposed several consistent systematic biases that significantly influence GenAI evaluation reliability and practical deployment viability.

\subsubsection{Systematic Optimism Bias}

The most prominent pattern is optimism in GenAI scoring. As noted above, Pista scored higher in all 22 cases (+1.70 average difference). This directional bias spans pitch quality and industry sectors, which suggests systematic rather than circumstantial influences.

Several factors likely contribute to this optimism bias. Training data composition may overrepresent successful startup examples, creating algorithmic tendencies toward positive assessment. Additionally, GenAI systems may lack the sophisticated risk evaluation capabilities that characterize experienced human evaluators, who integrate market realities and execution challenges developed through practical investment experience.

\subsubsection{Limited Discriminative Power}

A striking pattern emerged in score distribution analysis: 66.7\% of Pista evaluations (15/22 pitches) received identical overall scores of exactly 7.0/10. This substantial clustering indicates critical algorithmic limitations in quality differentiation capabilities. Rather than reflecting genuine quality similarities among evaluated pitches, this pattern suggests algorithmic convergence toward a default "safe" assessment value.

Such limited discrimination presents significant challenges for practical investment applications. Professional investors require granular quality distinctions to effectively prioritize opportunities and allocate limited evaluation resources. When most assessments converge on identical scores, the system fails to provide the nuanced discrimination essential for informed decision-making.

\begin{table}[ht]
    \centering
    \caption{Systematic Bias Analysis}
    \label{tab:bias-analysis}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Bias Category} & \textbf{Frequency} & \textbf{Impact Magnitude} & \textbf{Strategic Implications} \\
        \midrule
        GenAI Evaluation Optimism & 22/22 (100\%) & +1.70 points average & Systematic over-confidence \\
        Limited GenAI Discrimination & 15/22 (66.7\%) & Identical scores & Insufficient quality differentiation \\
        Commercial Conservatism & 22/22 (100\%) & Consistent lower scoring & Risk-aware assessment \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Dimensional Performance Analysis}
\label{subsec:dimensional}

\begin{table}[ht]
    \centering
    \caption{Pista Dimensional Performance Analysis}
    \label{tab:dimensional-analysis}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Dimension} & \textbf{Pista Average} & \textbf{Performance Characteristics} \\
        \midrule
        Problem-Solution Fit & 7.1/10 & Highest dimensional performance \\
        Business Model \& Market & 6.9/10 & Consistently strong assessment \\
        Pitch Quality & 6.8/10 & Stable evaluation patterns \\
        Team \& Execution & 6.6/10 & Comparatively lower performance \\
        \bottomrule
    \end{tabular}
\end{table}

Dimensional analysis exposed systematic performance variations across evaluation categories, revealing specific algorithmic strengths and limitations. Problem-Solution Fit achieved the highest average scores (7.1/10), suggesting that GenAI systems demonstrate particular effectiveness in analyzing business logic structures and solution viability assessments.

Conversely, Team \& Execution consistently received the lowest dimensional scores (6.6/10), indicating algorithmic challenges in assessing human capital factors. This performance gap likely reflects GenAI limitations in evaluating intangible elements such as leadership depth, team dynamics, and execution track records. These are factors that human evaluators often assess through experience based pattern recognition and interpersonal judgment.

\section{Case Study Analysis}

\begin{table}[ht]
    \centering
    \caption{Comparative Case Study Analysis}
    \label{tab:case-studies}
    \begin{tabular}{lp{3cm}cccp{4cm}}
        \toprule
        \textbf{Company} & \textbf{Description} & \textbf{Pista Score} & \textbf{W2V Score} & \textbf{Difference} & \textbf{Primary Evaluation Focus Difference} \\
        \midrule
        Coontent & Marketing automation B2B SaaS platform & 6.5/10 & 5.3/10 & +1.2 & GenAI: Innovation potential vs W2V: Market competition \\
        \midrule
        Serenity & GenAI-powered mental wellness platform & 6.9/10 & 6.1/10 & +0.8 & GenAI: Market opportunity vs W2V: Regulatory challenges \\
        \midrule
        CampoRapido & Agricultural technology platform & 7.0/10 & 4.5/10 & +2.5 & GenAI: Problem-solution fit vs W2V: Market readiness \\
        \midrule
        Corptech & Industrial technology platform & 8.0/10 & 6.0/10 & +2.0 & GenAI: Technical innovation vs W2V: Execution readiness \\
        \bottomrule
    \end{tabular}
\end{table}

Case study analysis across diverse industry sectors revealed three distinct patterns that illuminate fundamental evaluation differences. First, scoring differentials ranged from 0.8 to 2.5 points, with regulated industries (Serenity: +0.8) showing smaller gaps and agricultural technology (CampoRapido: +2.5) exhibiting the largest disparities. Second, evaluation focus differed systematically: Pista emphasized innovation potential and technical merit, while W2V prioritized practical deployment challenges including regulatory compliance, market readiness, and execution risks. Third, even high-performing pitches like Corptech, which received the highest scores from both systems (8.0 Pista, 6.0 W2V), maintained substantial scoring gaps (+2.0), confirming that evaluation differences stem from systematic philosophical divergences rather than pitch quality variations.

\section{Discussion and Deployment Implications}

The comparison revealed consistent differences between Pista and W2V scoring approaches. As shown above, Pista scores were higher on average, which suggests a tendency toward optimistic assessment. The following sections discuss practical implications and use cases.

\subsection{Systematic Bias Analysis}
\label{sec:bias-analysis}

The comparison reveals three main patterns in how Pista differs from W2V in its evaluation approach.

\subsubsection{Universal GenAI Optimism}
\label{subsec:optimism}

Pista's consistent higher scoring (22/22 pitches) was interpreted as suggesting a training bias toward positive examples; it was also considered to indicate insufficient risk assessment capabilities. This optimism was identified as limiting utility for investment decision contexts.

The universal directional bias indicates that GenAI systems may be fundamentally designed to highlight opportunities rather than assess risks. This could result from training data that emphasizes successful startup characteristics without adequate representation of failure patterns. Commercial evaluators, by contrast, integrate market realities and execution challenges that create more conservative assessments.

\subsubsection{Score Convergence Limitations}
\label{subsec:convergence}

The concentration of 66.7\% of scores at exactly 7.0/10 was identified as indicating critical algorithmic limitations in quality differentiation. This clustering was interpreted as suggesting the evaluation model defaults to a "safe" middle-high score regardless of actual pitch quality variations.

This convergence pattern creates several practical challenges. Investment decisions require discrimination between opportunities to allocate resources effectively. When most evaluations receive identical scores, the system fails to provide the granular assessment needed for professional deployment. The algorithmic tendency toward convergence may reflect conservative design choices that prioritize consistency over discrimination.

\subsubsection{Risk Assessment Variations}
\label{subsec:risk}

Maximum disagreements occurred in technology and service platforms where commercial evaluators emphasized execution risks and market competition factors that GenAI evaluation appears to underweight.

GenAI systems focus on innovation potential and technical merit while underweighting execution challenges and competitive dynamics. Commercial platforms incorporate broader risk factors developed through investment experience. This difference reflects fundamental evaluation philosophy variations rather than calibration issues.

\subsection{Deployment Strategy Implications}
\label{sec:deployment}

The systematic differences suggest complementary rather than competing deployment strategies. Given GenAI's operational advantages (speed, cost, accessibility) and limitations (optimism bias, poor discrimination), strategic deployment becomes crucial.

\subsubsection{Appropriate GenAI Applications}
\label{subsec:ai-applications}

GenAI evaluation systems demonstrate clear advantages for specific use cases that align with their operational strengths and philosophical characteristics:

\textbf{Early-stage entrepreneur feedback and pitch development support}: The systematic optimism bias becomes advantageous for encouraging entrepreneurs and providing constructive feedback without discouraging innovation attempts.

\textbf{High-volume initial screening where broad categorization suffices}: The speed and cost advantages enable processing large numbers of pitches where precise discrimination is less critical than general quality assessment.

\textbf{Educational contexts where encouraging feedback promotes learning}: The consistent scoring patterns provide stable feedback that supports learning environments without creating discouraging assessment variations.

\subsubsection{Commercial Evaluation Requirements}
\label{subsec:commercial-requirements}

Commercial evaluation platforms demonstrate capabilities that remain essential for professional investment contexts:

\textbf{Investment decision-making requiring realistic risk assessment}: The broader score distribution and conservative assessment approach align with professional investment requirements for risk evaluation.

\textbf{Applications demanding granular quality discrimination}: The varied scoring patterns enable the discrimination needed for resource allocation and opportunity prioritization in competitive markets.

\textbf{Complex market sectors}: Technology and B2B services may benefit from the domain expertise that platforms like W2V provide through experienced evaluators.

\subsubsection{Hybrid Strategy Optimization}
\label{subsec:hybrid}

Using GenAI tools like Pista for initial feedback and platforms like W2V for more detailed assessment could combine the speed of GenAI with the nuanced evaluation of experienced assessors.

The hybrid strategy addresses both system limitations and advantages. GenAI systems handle high-volume screening efficiently while commercial systems provide the discrimination and risk assessment needed for investment decisions. This complementary approach maximizes overall evaluation system effectiveness.

\subsection{Evaluation Quality vs. Operational Efficiency Trade-offs}
\label{sec:tradeoffs}

The evaluation reveals a fundamental trade-off between operational efficiency and assessment quality. GenAI evaluation provides substantial operational advantages but at the cost of evaluation precision and realistic risk assessment.

\subsubsection{Operational Advantages}
\label{subsec:advantages}

GenAI's speed (30-60 seconds), cost (\$0.10-0.15), and availability (24/7) enable applications impossible with traditional evaluation methods, democratizing access to startup feedback. These advantages create entirely new use cases that were previously economically unfeasible.

The operational efficiency enables applications such as real-time pitch feedback during development, continuous iteration support, and accessible evaluation for entrepreneurs who cannot afford professional assessment services. This democratization effect represents a significant value creation opportunity.

\subsubsection{Quality Limitations}
\label{subsec:limitations}

Score convergence and optimism bias severely limit GenAI utility for contexts requiring discriminative assessment or realistic risk evaluation. These limitations constrain professional deployment opportunities where quality assessment is critical.

The quality limitations become particularly problematic when AI evaluation results influence significant decisions. Investment contexts require accurate risk assessment and quality discrimination that current GenAI systems cannot provide reliably. This constrains professional deployment until these limitations are addressed.

\subsubsection{Strategic Trade-off Management}
\label{subsec:strategy}

These differences suggest GenAI evaluation and traditional platforms serve different purposes, with each being more suitable for specific use cases rather than directly competing.

Successful deployment requires matching GenAI capabilities with appropriate use cases while using commercial evaluation where quality requirements exceed GenAI capabilities. This strategic approach enables value creation through GenAI advantages while maintaining quality standards where needed.

\section{Threats to Validity}
\label{sec:threats}

This section reflects on limitations that may affect interpretation of results.

\subsection{Internal Validity}
The evaluation uses identical pitch content across systems, but prompts and calibration choices may influence outcomes. We reduced this risk with rubric anchors, evidence gating, and a fixed temperature, yet residual effects may remain. Future work can ablate each calibration component to measure its impact.

\subsection{Construct Validity}
The four-dimension rubric approximates startup quality but cannot capture all factors. Some human judgments, such as leadership depth and execution history, are hard to encode. We mitigated this by adding a required Q\&A step to collect missing evidence before scoring.

\subsection{External Validity}
The dataset contains 22 university competition pitches. Results may not generalize to all sectors or later-stage ventures. Expanding the dataset and including investor-written memos would improve generalizability.

\subsection{Conclusion Validity}
We report descriptive statistics and qualitative patterns. Larger samples and rank-correlation tests would strengthen claims about dispersion and agreement. We recommend Kendall's $\tau$ for future studies.

\section{Summary}
\label{sec:summary}

This comparative evaluation examined systematic differences between GenAI and commercial startup assessment methodologies through analysis of 22 university entrepreneurship competition pitches. The investigation revealed fundamental divergences in evaluation philosophy and practical capabilities between automated and human expert assessment approaches.

\textbf{Principal Findings}:
\begin{itemize}
    \item Universal optimism bias: Pista scored higher than W2V in all cases without exception (22/22 pitches)
    \item Substantial scoring differential: +1.70 average point difference (Pista: 6.81/10, W2V: 5.11/10)
    \item Algorithmic convergence limitation: 66.7\% of GenAI evaluations clustered at identical 7.0/10 scores
    \item Commercial platform demonstrated broader discrimination: 4.3-6.4 scoring range with higher variance
    \item Systematic GenAI tendency toward middle-high score convergence independent of actual quality variations
\end{itemize}

The findings suggest that GenAI evaluation systems like Pista and platforms like W2V have different strengths, making them suitable for different purposes in startup assessment.
