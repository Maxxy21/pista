\chapter{Background and Related Work}
\label{ch:soa}
This chapter reviews previous research on pitch evaluation and GenAI-based assessment systems. The problems with traditional pitch evaluation methods are first examined, followed by an exploration of how GenAI systems address these issues. The need for better evaluation methods is demonstrated, and current GenAI evaluation systems and their capabilities are examined.

\section{Limitations of Traditional Pitch Evaluation Methods}
\label{sec:traditional-methods}

This section explains the main problems with traditional evaluation methods. Three key issues are identified: poor scalability, limited reliability, and restricted access for many entrepreneurs.

For decades, startup pitch evaluation has relied on experts using their judgment and experience. Most evaluation happens through expert panels or standardized scoring frameworks. Both methods remain important in how investors make decisions. However, research shows three main problems with these traditional approaches: they don't scale well, they're not consistent, and they're hard for many entrepreneurs to access. Both expert panels and standardized frameworks are examined to better understand these challenges.

\subsection{Expert Panels}\label{subsec:expert-panels}
Expert panels use industry professionals, investors, and entrepreneurs to evaluate startup pitches. These panels watch live presentations and give feedback based on specific criteria about market opportunity and team quality. Proponents of this approach argue that human insight is valuable for judging aspects that are difficult to measure computationally, such as founder passion, confidence levels, and presentation quality.

However, research shows that expert panels have consistency problems. Gius (2024) looked at data from 67 startup competitions and found that expert judges often disagree when evaluating the same pitches \cite{Gius2024}. This disagreement is actually common and can even predict which startups will succeed. The variation in expert opinions raises questions about whether these evaluations are reliable and fair.

Bias is another serious problem with expert panels. Balachandra et al. (2019) found clear gender differences in how investors evaluate pitches \cite{Balachandra2019}. They showed that the same business idea gets different treatment depending on who presents it. Female entrepreneurs get more questions about risks and potential problems, while male entrepreneurs get questions focused on growth opportunities. This shows that even experienced evaluators have unconscious biases that affect their judgments.

\subsection{Standardized Frameworks}\label{subsec:standardized-frameworks}
To fix the consistency problems with expert panels, many organizations have created standardized evaluation criteria and scoring systems. These frameworks usually look at four main areas:
\begin{itemize}
    \item Market opportunity (market size, growth potential)
    \item Team capability (team experience, diversity)
    \item Product innovation (uniqueness, technical feasibility)
    \item Business model (revenue streams, scalability)
\end{itemize}

Structured scoring systems have been shown to provide better consistency between evaluators compared to unstructured approaches \cite{Tsay2021VISUALSDI}. However, challenges still exist. Gompers et al. (2020) surveyed venture capital firms and found huge differences in how they evaluate startups \cite{Gompers2020}. There's no single evaluation framework that everyone in the industry uses. Standardization helps, but the main evaluation problems are still there.

These four areas are commonly emphasized in standardized frameworks due to their demonstrated predictive value for startup success \cite{Kalvapalle2024}.

\subsection{Key Limitations}\label{subsec:key-limitations}
Traditional evaluation methods face three main problems that work together to make evaluation difficult:

\begin{enumerate}
    \item \textbf{Scalability}: Human evaluators can only review so many pitches before they get tired and make worse decisions. Hirshleifer et al. (2019) showed that financial analysts become less accurate when they have to evaluate too many things in one day \cite{Hirshleifer2019}. When analysts get tired, they start making more basic mistakes. This means that during busy periods, the quality of evaluations goes down because people can't handle the workload.

    \item \textbf{Geographic Access Barriers}: Most expert evaluators live in specific cities and regions, making it hard for startups in other areas to get good feedback. Colombo et al. (2019) studied how distance affects startup funding in Europe \cite{Colombo2019}. They found that startups far from major investment centers are less likely to seek funding because they feel like they can't access the right evaluators. This clustering of experts in certain places creates problems for entrepreneurs in remote areas who want quality evaluation.

    \item \textbf{Consistency Challenges}: Traditional evaluation methods have trouble balancing subjective feelings with objective facts in a consistent way. Different evaluators put different levels of importance on personal impressions versus hard data when predicting which startups will succeed \cite{Tsay2021VISUALSDI}. This inconsistency makes it hard to know what really matters in evaluation.
\end{enumerate}

These problems hurt early-stage startups and underrepresented founders the most. They demonstrate the need for more systematic and fair ways to evaluate pitches. Traditional methods struggle to be both consistent and contextual at the same time. GenAI systems might help solve this problem because they can apply the same criteria consistently while providing detailed feedback. These limitations motivated the system design described in Chapter~\ref{ch:problem-solution}.

\section{GenAI-Driven Evaluation Systems}
\label{sec:GenAI-systems}

This section reviews GenAI-based evaluation systems and compares their capabilities and limitations to traditional methods.

The problems with traditional evaluation methods have led to growing interest in GenAI-based systems. These new systems use large language models and automated text analysis to solve the scalability, consistency, and accessibility problems described above.

\subsection{Current Approaches}\label{subsec:current-approaches}
The growing interest in GenAI evaluation has created several different approaches. Each one tries to solve different parts of the traditional evaluation problems:

\begin{itemize}
    \item \textbf{Machine Learning-Based Assessment}: Researchers have built systems that learn from data about past startups to make better evaluation decisions. Arroyo et al. (2019) showed how machine learning can help venture capital firms make decisions \cite{Arroyo2019}. These systems look at patterns in successful and failed startups to figure out what makes startups succeed.

    \item \textbf{Text-Based Evaluation Systems}: These systems use automated text analysis to read and evaluate pitch content, business plans, and presentation materials. They can judge things like how clear the value proposition is, how well the team understands the market, and how they position themselves against competitors.

    \item \textbf{GenAI Evaluation Platforms}: Several GenAI-powered evaluation systems have emerged in this area. Winds2Ventures (W2V), developed within the same research group, provides startup evaluations based on multiple criteria and gives structured feedback \cite{w2v}. This platform was made available for comparison through supervisor facilitation, enabling benchmarking against different GenAI evaluation approaches.

    \item \textbf{Large Language Model Integration}: Recent advances in large language models have made it possible to create more sophisticated systems that understand text better. These systems can read pitch content and provide detailed feedback on many different aspects of the pitch at once.
\end{itemize}


\subsection{Performance Considerations}\label{subsec:performance-considerations}
GenAI systems have several advantages compared to traditional evaluation methods:

\begin{itemize}
    \item \textbf{Scalability}: GenAI systems can evaluate many pitches without getting tired like human evaluators do. This means they can maintain consistent quality even when processing large volumes of pitches.

    \item \textbf{Consistency}: Automated systems apply the same evaluation criteria every time, which reduces the variability that happens with human expert panels.

    \item \textbf{Accessibility}: GenAI-based evaluation can provide quality feedback to entrepreneurs anywhere in the world. This solves the access problems that come with expert panels that are only available in certain locations.

    \item \textbf{Structured Feedback}: GenAI systems provide detailed, organized feedback on multiple aspects of a pitch. This helps entrepreneurs understand exactly what they need to improve.
\end{itemize}

However, GenAI systems still have important limitations. Humans are still needed to check the context and make final investment decisions \cite{Steyvers2024}. GenAI systems might have trouble understanding subtle aspects of entrepreneurship that require deep knowledge about specific industries or market dynamics that go beyond what they can learn from text.

\subsection{Diversity in GenAI Evaluation Approaches}\label{subsec:GenAI-evaluation-diversity}
As GenAI evaluation systems develop, different platforms have appeared with different ways of assessing pitches. Understanding these differences helps position the comparison research within the broader world of automated evaluation systems. GenAI systems differ in their evaluation criteria, how their algorithms work, and how they provide feedback. Some platforms focus on analyzing many different criteria at once, while others emphasize specific areas like market validation or team assessment. These differences create opportunities to study how different GenAI evaluation approaches compare when they look at the same pitches.

The growing diversity of GenAI evaluation systems creates both opportunities and challenges. This diversity motivates comparative studies that examine how different automated approaches agree or disagree in their assessments. This comparison guides the evaluation design in Chapter~\ref{ch:evaluation}.



